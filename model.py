# -*- coding: utf-8 -*-
"""hindihnd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fhrr9GAKhePh8NGOoSzzsiJWzPjnu53l
"""

import pandas as pd
import numpy as np
import keras
from keras.models import Sequential,Model
from keras.layers import Dense, Dropout, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D
from sklearn.metrics import confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

#Loading data file from google drive
data=pd.read_csv('drive/My Drive/dataset/data.csv')
print(data)

dataset=np.array(data)
np.random.shuffle(dataset)

X=dataset
Y=dataset
X=X[:, 0:1024]
Y=Y[:,1024]

#Splitting data for test and train 
X_train=X[0:70000,:]
x_test=X[70000:72001,:]
#Data normalization
X_train=X_train/255
x_test=x_test/255

#Reshape
Y=Y.reshape(Y.shape[0],1)
Y_train=Y[0:70000,:]
Y_train=Y_train.T
y_test=Y[70000:72001,:]
y_test=y_test.T

print(X_train.shape)
print(Y_train.shape)
print(x_test.shape)
print(y_test.shape)

#one-hot encode
train_y=keras.utils.to_categorical(Y_train)
test_y=keras.utils.to_categorical(y_test)
train_y=train_y.reshape(train_y.shape[1],train_y.shape[2])
test_y=test_y.reshape(test_y.shape[1],test_y.shape[2])
X_train= X_train.reshape(X_train.shape[0],32,32,1)
x_test=x_test.reshape(x_test.shape[0],32,32,1)

print(X_train.shape)
print(train_y.shape)

#create model
def create_model():
    """
    Inputs:
        None
    Outputs:
        model: compiled keras model
    """
    model=Sequential()
    input_layer=Input(shape=(X_train.shape[1:]))
    h1=Conv2D(74, kernel_size=(3, 3), activation='relu')(input_layer)
    h2=Conv2D(74, (3, 3), activation='relu')(h1)
    h3=MaxPooling2D(pool_size=(3, 3))(h2)
    h4=Flatten()(h3)
    h5=Dense(150, activation='sigmoid')(h4)
    output_layer=Dense(37,activation='softmax')(h5)
    model=Model(inputs=[input_layer], output=[output_layer])
    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
    return model

model=create_model()

model.summary()

history = model.fit(X_train, train_y, epochs=5, validation_data=(x_test, test_y),batch_size=200)

scores=model.evaluate(x_test, test_y)

print(100-scores[1]*100)

model.save('devanagri1.h5',overwrite='true')

